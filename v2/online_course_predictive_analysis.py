# -*- coding: utf-8 -*-
"""Online Course Predictive Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1od49vVlMvfruDcdqU60acSuwc4hOKpN8

## Import Package
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split

"""Mengimpor beberapa package yang akan digunakan untuk menyimpan data dalam tabel, membuat visualisasi data, encoding data, scalling data, pembagian dataset latih dan dataset uji

## Load Dataset
"""

data = pd.read_csv('/content/online_course_engagement_data.csv')

"""Mengimpor data dari file csv

## Data Exploration
"""

categorical_features = data.select_dtypes(include=['object']).columns
numerical_features = data.select_dtypes(include=['int64', 'float64']).columns

"""Memisahkan kategorikal dan numerikal fitur"""

data.head()

"""Dari data yang yang telah di load, diketahui data tersebut memiliki 9 kolom yaitu UserID, CourseCategory, TimeSpentOnCourse, NumberOfVideosWatched, NumberOfQuizzesTaken, QuizScores, CompletionRate, DeviceType, CourseCompletion"""

data.info()

"""Dari informasi diatas didapatkan bahwa data tersebut berjumlah 9000 dengan tidak ada data yang bernilai null"""

data[numerical_features].describe()

"""Dari summary fitur numerik diatas dapat ditarik beberapa informasi
- Rata-rata user menghabiskan waktu di kelas adalah 50 jam
- Rata-rata video yang ditonton oleh user yaitu 10 video
- Rata-rata user mengerjakan kuis sebanyak 5 kali
- Rata-rata skor kuis user adalah 74
- Rata-rata completion rate user adalah 50%
"""

data[categorical_features].astype('object').describe(include='all')

"""Dari summary fitur kategori diatas dapat ditarik beberapa informasi
- Kategori kelas yang paling banyak dibeli adalah kategori bisnis

### Proportion Course Completion
"""

round(data['CourseCompletion'].value_counts(normalize=True) * 100, 2)

"""Dari data diatas terlihat bahwa 60.36% user tidak selesai kelas dan 39.64% menyelesaikan kelas

### Correlation Matrix
"""

plt.figure(figsize=(12, 8))
corr_matrix = data[numerical_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""Dari matriks korelasi diatas diketahui bahwa Top 3 hal yang paling mempengaruhi CourseCompletion adalah CompletionRate, QuizScores, dan NumberOfQuizzesTaken

## Data Preparation
"""

data.drop('UserID', axis=1, inplace=True)

"""Menghapus kolom UserID karena tidak memiliki karakteristik sehingga tidak akan digunakan dalam permodelan

### Label Encoding
"""

encoder = OneHotEncoder(sparse_output=False)
encoded_features = encoder.fit_transform(data[categorical_features])

encoded_data = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))

data = data.join(encoded_data)
data = data.drop(categorical_features, axis=1)
data.head()

"""Melakukan encoding kategorikal fitur agar dapat diproses oleh machine learning. Proses encoding dilakukan dengan One Hot Encoder agar algoritma machine learning memahami kategori tanpa menganggap adanya urutan atau hubungan numerik antara kategori tersebut.

### Data Split
"""

X = data.drop(["CourseCompletion"],axis =1)
y = data["CourseCompletion"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

"""Membagi data menjadi data latih dan data uji dengan perbandingan 80% data latih dan 20% data uji

### Standarisasi
"""

scaler = StandardScaler()
scaler.fit(X_train)
X_train[X_train.columns] = scaler.transform(X_train)

"""Karena nilai setiap fitur memiliki skala yang berbeda maka dibutuhkan sehingga dibutuhkan proses scalling sehingga memiliki skala yang sama data distribusi nilai yang seragam. Dengan data yang diskalakan juga dapat meningkatkan performa pelatihan model machine learning dapat lebih cepat menemukan solusi optimal dan menghindari bias terhadap fitur dengan skala yang lebih besar. StandardScaler mengubah fitur-fitur dalam dataset sehingga memiliki distribusi dengan mean 0 dan standard deviation 1"""

X_train.head()

"""Berikut preview data latih yang telah dilakukan scalling. terlihat bahwa setiap fitur memiliki rentang skala yang seragam"""

X_test[X_test.columns] = scaler.transform(X_test)

"""Scalling data juga dilakukan di data uji untuk input pengujian model."""

X_test.head()

"""Terlihat bahwa setiap fitur dari data uji sudah memiliki skala yang seragam

## Model Development
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

"""Kode diatas digunakan untuk mengimpor package yang diperlukan dalam proses pelatihan model yaitu beberapa jenis model klasifikasi dan metrik evaluasi yaitu skor akurasi"""

models = pd.DataFrame(index=['train_accuracy', 'test_accuracy'],
                      columns=['KNN', 'DecisionTree', 'RandomForest', 'Boosting'])

"""Kode diatas digunakan untuk menyiapkan tabel yang akan digunakan untuk melihat perbandingan akurasi dari model yang akan dilatih"""

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

models.loc['train_accuracy','KNN'] = accuracy_score(y_pred = knn.predict(X_train), y_true=y_train)
models.loc['test_accuracy','KNN'] = accuracy_score(y_pred = knn.predict(X_test), y_true=y_test)

"""Dilakukan proses pelatihan dengan algoritma KNN dengan nilai n = 3. Model yang telah dilatih kemudian dievaluasi menggunakan data latih dan dihitung skor akurasinya. Setelah itu dimasukkan kedalam tabel yang telah disiapkan."""

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
models.loc['train_accuracy','DecisionTree'] = accuracy_score(y_pred = dt.predict(X_train), y_true=y_train)
models.loc['test_accuracy','DecisionTree'] = accuracy_score(y_pred = dt.predict(X_test), y_true=y_test)

"""Dilakukan proses pelatihan dengan algoritma Decision Tree. Model yang telah dilatih kemudian dievaluasi menggunakan data latih dan dihitung skor akurasinya. Setelah itu dimasukkan kedalam tabel yang telah disiapkan."""

rf = RandomForestClassifier(n_estimators = 100)
rf.fit(X_train, y_train)
models.loc['train_accuracy','RandomForest'] = accuracy_score(y_pred = rf.predict(X_train), y_true=y_train)
models.loc['test_accuracy','RandomForest'] = accuracy_score(y_pred = rf.predict(X_test), y_true=y_test)

"""Dilakukan proses pelatihan dengan algoritma Random Forest dengan nilai n estimator 100. Model yang telah dilatih kemudian dievaluasi menggunakan data latih dan dihitung skor akurasinya. Setelah itu dimasukkan kedalam tabel yang telah disiapkan."""

gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
models.loc['train_accuracy','Boosting'] = accuracy_score(y_pred = gb.predict(X_train), y_true=y_train)
models.loc['test_accuracy','Boosting'] = accuracy_score(y_pred = gb.predict(X_test), y_true=y_test)

"""Dilakukan proses pelatihan dengan algoritma Gradient Boosting. Model yang telah dilatih kemudian dievaluasi menggunakan data latih dan dihitung skor akurasinya. Setelah itu dimasukkan kedalam tabel yang telah disiapkan."""

models

"""Dari tabel skor akurasi beberapa model yang telah dilatih didapatkan bahwa Algoritma Gradient Boosting memiliki performa yang paling baik karena mengahasilkan akurasi fit tertinggi antara akurasi data latih dan data uji."""

importances = gb.feature_importances_

feature_names = X.columns
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

importance_df

"""Dari model terbaik dilakukan perangkingan fitur yang paling penting dalam menentukan CourseCompletion yaitu top 3 fitur nya adalah QuizScores, NumberOfQuizzesTaken, dan CompletionRate"""